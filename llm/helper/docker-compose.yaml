version: '3.9'

services:
  ollama:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: ollama-llama3.1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
              device_ids: ['GPU-d6358919-62a6-3996-79cb-4875a5dd23c8']
    networks:
      - llama
    volumes:
      - ollama-llama-data:/root/.ollama
    ports:
      - "11435:11434"

volumes:
  ollama-llama-data:
    driver: local

networks:
  llama:
    driver: bridge

